{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556f421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c90f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_hid(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98f2706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_hid(a):\n",
    "    return (a > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165deb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_out(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))  # Compute exponentials\n",
    "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)    # Summation & division\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_hid(a):\n",
    "    \n",
    "    return (a > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73354f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters):\n",
    "    act_preact={\"h0\":X}\n",
    "    L=len(parameters)//2\n",
    "    \n",
    "    for i in range (1,L+1):\n",
    "        w=parameters[f\"W{i}\"]\n",
    "        b=parameters[f\"b{i}\"]\n",
    "        a=np.dot(w,act_preact[f\"h{i-1}\"]) +b\n",
    "        if i==L:\n",
    "            h=act_out(a)\n",
    "        else :\n",
    "            h=act_hid(a)\n",
    "        act_preact[f\"a{i}\"]=a\n",
    "        act_preact[f\"h{i}\"] =h\n",
    "    return act_preact[f\"h{L}\"],act_preact\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e504579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(Y,act_preact,parameters):\n",
    "    grad_w_b={}\n",
    "    L=len(parameters)//2\n",
    "    m=Y.shape[1]\n",
    "    dZ=act_preact[f\"h{L}\"]-Y\n",
    "    grad_w_b[f\"dW{L}\"]=(1/m)*np.dot(dZ,act_preact[f\"h{L-1}\"].T)\n",
    "    grad_w_b[f\"db{L}\"]=(1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "\n",
    "    for i in range(L-1,0,-1):\n",
    "        dA=np.dot(parameters[f\"W{i+1}\"].T,dZ)\n",
    "\n",
    "        dZ=dA*der_hid(act_preact[f\"a{i}\"])\n",
    "        grad_w_b[f\"dW{i}\"] = (1/m) * np.dot(dZ, act_preact[f\"h{i-1}\"].T)\n",
    "        grad_w_b[f\"db{i}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    return grad_w_b\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb5dea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_sgd(parameters,grad_w_b,eta):\n",
    "    L = len(parameters) // 2\n",
    "    for i in range(1, L+1):\n",
    "        parameters[f\"W{i}\"] -= eta * grad_w_b[f\"dW{i}\"]\n",
    "        parameters[f\"b{i}\"] -= eta * grad_w_b[f\"db{i}\"]\n",
    "    return parameters\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43772334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_momentum(parameters, grad_w_b, state, eta, beta):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L+1):\n",
    "        state.setdefault(f\"vW{l}\", np.zeros_like(parameters[f\"W{l}\"]))\n",
    "        state.setdefault(f\"vb{l}\", np.zeros_like(parameters[f\"b{l}\"]))\n",
    "        \n",
    "        state[f\"vW{l}\"] = beta * state[f\"vW{l}\"] + eta * grad_w_b[f\"dW{l}\"]\n",
    "        state[f\"vb{l}\"] = beta * state[f\"vb{l}\"] + eta * grad_w_b[f\"db{l}\"]\n",
    "        \n",
    "        parameters[f\"W{l}\"] -= state[f\"vW{l}\"]\n",
    "        parameters[f\"b{l}\"] -= state[f\"vb{l}\"]\n",
    "    return parameters, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7bc426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_nesterov(parameters, grad_w_b_w_b, state, eta, beta):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L+1):\n",
    "        vW_prev = state.get(f\"vW{l}\", np.zeros_like(parameters[f\"W{l}\"]))\n",
    "        vb_prev = state.get(f\"vb{l}\", np.zeros_like(parameters[f\"b{l}\"]))\n",
    "        \n",
    "        state[f\"vW{l}\"] = beta * vW_prev + eta * grad_w_b_w_b[f\"dW{l}\"]\n",
    "        state[f\"vb{l}\"] = beta * vb_prev + eta * grad_w_b_w_b[f\"db{l}\"]\n",
    "        \n",
    "        parameters[f\"W{l}\"] -= (beta * vW_prev + (1-beta) * state[f\"vW{l}\"])\n",
    "        parameters[f\"b{l}\"] -= (beta * vb_prev + (1-beta) * state[f\"vb{l}\"])\n",
    "    return parameters, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3285e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_rmsprop(parameters, grad_w_b_w_b, state, eta, beta, epsilon=1e-8):\n",
    "   \n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L+1):\n",
    "        state.setdefault(f\"sW{l}\", np.zeros_like(parameters[f\"W{l}\"]))\n",
    "        state.setdefault(f\"sb{l}\", np.zeros_like(parameters[f\"b{l}\"]))\n",
    "        \n",
    "        state[f\"sW{l}\"] = beta * state[f\"sW{l}\"] + (1-beta) * (grad_w_b_w_b[f\"dW{l}\"] ** 2)\n",
    "        state[f\"sb{l}\"] = beta * state[f\"sb{l}\"] + (1-beta) * (grad_w_b_w_b[f\"db{l}\"] ** 2)\n",
    "        \n",
    "        parameters[f\"W{l}\"] -= eta * grad_w_b_w_b[f\"dW{l}\"] / (np.sqrt(state[f\"sW{l}\"]) + epsilon)\n",
    "        parameters[f\"b{l}\"] -= eta * grad_w_b_w_b[f\"db{l}\"] / (np.sqrt(state[f\"sb{l}\"]) + epsilon)\n",
    "    return parameters, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e68ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_adam(parameters, grad_w_b_w_b, state, eta, beta1, beta2, epsilon, t):\n",
    "   \n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L+1):\n",
    "        state.setdefault(f\"vW{l}\", np.zeros_like(parameters[f\"W{l}\"]))\n",
    "        state.setdefault(f\"vb{l}\", np.zeros_like(parameters[f\"b{l}\"]))\n",
    "        state.setdefault(f\"sW{l}\", np.zeros_like(parameters[f\"W{l}\"]))\n",
    "        state.setdefault(f\"sb{l}\", np.zeros_like(parameters[f\"b{l}\"]))\n",
    "        \n",
    "        state[f\"vW{l}\"] = beta1 * state[f\"vW{l}\"] + (1 - beta1) * grad_w_b_w_b[f\"dW{l}\"]\n",
    "        state[f\"vb{l}\"] = beta1 * state[f\"vb{l}\"] + (1 - beta1) * grad_w_b_w_b[f\"db{l}\"]\n",
    "        state[f\"sW{l}\"] = beta2 * state[f\"sW{l}\"] + (1 - beta2) * (grad_w_b_w_b[f\"dW{l}\"] ** 2)\n",
    "        state[f\"sb{l}\"] = beta2 * state[f\"sb{l}\"] + (1 - beta2) * (grad_w_b_w_b[f\"db{l}\"] ** 2)\n",
    "        \n",
    "        vW_corr = state[f\"vW{l}\"] / (1 - beta1**t)\n",
    "        vb_corr = state[f\"vb{l}\"] / (1 - beta1**t)\n",
    "        sW_corr = state[f\"sW{l}\"] / (1 - beta2**t)\n",
    "        sb_corr = state[f\"sb{l}\"] / (1 - beta2**t)\n",
    "        \n",
    "        parameters[f\"W{l}\"] -= eta * vW_corr / (np.sqrt(sW_corr) + epsilon)\n",
    "        parameters[f\"b{l}\"] -= eta * vb_corr / (np.sqrt(sb_corr) + epsilon)\n",
    "    return parameters, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "792d5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_nadam(parameters, grad_w_b_w_b, state, eta, beta1, beta2, epsilon, t):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L+1):\n",
    "        state.setdefault(f\"vW{l}\", np.zeros_like(parameters[f\"W{l}\"]))\n",
    "        state.setdefault(f\"vb{l}\", np.zeros_like(parameters[f\"b{l}\"]))\n",
    "        state.setdefault(f\"sW{l}\", np.zeros_like(parameters[f\"W{l}\"]))\n",
    "        state.setdefault(f\"sb{l}\", np.zeros_like(parameters[f\"b{l}\"]))\n",
    "        \n",
    "        state[f\"vW{l}\"] = beta1 * state[f\"vW{l}\"] + (1-beta1) * grad_w_b_w_b[f\"dW{l}\"]\n",
    "        state[f\"vb{l}\"] = beta1 * state[f\"vb{l}\"] + (1-beta1) * grad_w_b_w_b[f\"db{l}\"]\n",
    "        state[f\"sW{l}\"] = beta2 * state[f\"sW{l}\"] + (1-beta2) * (grad_w_b_w_b[f\"dW{l}\"] ** 2)\n",
    "        state[f\"sb{l}\"] = beta2 * state[f\"sb{l}\"] + (1-beta2) * (grad_w_b_w_b[f\"db{l}\"] ** 2)\n",
    "        \n",
    "        vW_corr = state[f\"vW{l}\"] / (1 - beta1**t)\n",
    "        vb_corr = state[f\"vb{l}\"] / (1 - beta1**t)\n",
    "        sW_corr = state[f\"sW{l}\"] / (1 - beta2**t)\n",
    "        sb_corr = state[f\"sb{l}\"] / (1 - beta2**t)\n",
    "        \n",
    "        parameters[f\"W{l}\"] -= eta * (beta1 * vW_corr + (1-beta1) * grad_w_b_w_b[f\"dW{l}\"] / (1-beta1**(t+1))) / (np.sqrt(sW_corr) + epsilon)\n",
    "        parameters[f\"b{l}\"] -= eta * (beta1 * vb_corr + (1-beta1) * grad_w_b_w_b[f\"db{l}\"] / (1-beta1**(t+1))) / (np.sqrt(sb_corr) + epsilon)\n",
    "    return parameters, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a16a1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_nadam(parameters, grad_w_b_w_b, state, eta, beta1, beta2, epsilon, t):\n",
    "    \"\"\"\n",
    "    Updates parameters using Nadam.\n",
    "    Nadam combines Adam with Nesterov momentum.\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L+1):\n",
    "        state.setdefault(f\"vW{l}\", np.zeros_like(parameters[f\"W{l}\"]))\n",
    "        state.setdefault(f\"vb{l}\", np.zeros_like(parameters[f\"b{l}\"]))\n",
    "        state.setdefault(f\"sW{l}\", np.zeros_like(parameters[f\"W{l}\"]))\n",
    "        state.setdefault(f\"sb{l}\", np.zeros_like(parameters[f\"b{l}\"]))\n",
    "        \n",
    "        state[f\"vW{l}\"] = beta1 * state[f\"vW{l}\"] + (1-beta1) * grad_w_b_w_b[f\"dW{l}\"]\n",
    "        state[f\"vb{l}\"] = beta1 * state[f\"vb{l}\"] + (1-beta1) * grad_w_b_w_b[f\"db{l}\"]\n",
    "        state[f\"sW{l}\"] = beta2 * state[f\"sW{l}\"] + (1-beta2) * (grad_w_b_w_b[f\"dW{l}\"] ** 2)\n",
    "        state[f\"sb{l}\"] = beta2 * state[f\"sb{l}\"] + (1-beta2) * (grad_w_b_w_b[f\"db{l}\"] ** 2)\n",
    "        \n",
    "        vW_corr = state[f\"vW{l}\"] / (1 - beta1**t)\n",
    "        vb_corr = state[f\"vb{l}\"] / (1 - beta1**t)\n",
    "        sW_corr = state[f\"sW{l}\"] / (1 - beta2**t)\n",
    "        sb_corr = state[f\"sb{l}\"] / (1 - beta2**t)\n",
    "        \n",
    "        parameters[f\"W{l}\"] -= eta * (beta1 * vW_corr + (1-beta1) * grad_w_b_w_b[f\"dW{l}\"] / (1-beta1**(t+1))) / (np.sqrt(sW_corr) + epsilon)\n",
    "        parameters[f\"b{l}\"] -= eta * (beta1 * vb_corr + (1-beta1) * grad_w_b_w_b[f\"db{l}\"] / (1-beta1**(t+1))) / (np.sqrt(sb_corr) + epsilon)\n",
    "    return parameters, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1239b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, optimizer, optimizer_state, eta, t=1, hyperparams={}):\n",
    "    \n",
    "    if optimizer == \"sgd\":\n",
    "        parameters = update_parameters_sgd(parameters, grads, eta)\n",
    "    elif optimizer == \"momentum\":\n",
    "        parameters, optimizer_state = update_parameters_momentum(parameters, grads, optimizer_state, eta, beta=hyperparams.get(\"beta\", 0.9))\n",
    "    elif optimizer == \"nesterov\":\n",
    "        parameters, optimizer_state = update_parameters_nesterov(parameters, grads, optimizer_state, eta, beta=hyperparams.get(\"beta\", 0.9))\n",
    "    elif optimizer == \"rmsprop\":\n",
    "        parameters, optimizer_state = update_parameters_rmsprop(parameters, grads, optimizer_state, eta, beta=hyperparams.get(\"beta\", 0.9), epsilon=hyperparams.get(\"epsilon\", 1e-8))\n",
    "    elif optimizer == \"adam\":\n",
    "        parameters, optimizer_state = update_parameters_adam(parameters, grads, optimizer_state, eta,\n",
    "                                                             beta1=hyperparams.get(\"beta1\", 0.9),\n",
    "                                                             beta2=hyperparams.get(\"beta2\", 0.999),\n",
    "                                                             epsilon=hyperparams.get(\"epsilon\", 1e-8),\n",
    "                                                             t=t)\n",
    "    elif optimizer == \"nadam\":\n",
    "        parameters, optimizer_state = update_parameters_nadam(parameters, grads, optimizer_state, eta,\n",
    "                                                              beta1=hyperparams.get(\"beta1\", 0.9),\n",
    "                                                              beta2=hyperparams.get(\"beta2\", 0.999),\n",
    "                                                              epsilon=hyperparams.get(\"epsilon\", 1e-8),\n",
    "                                                              t=t)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown optimizer!\")\n",
    "    return parameters, optimizer_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5726412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initializes parameters for an L-layer network.\n",
    "    \n",
    "    Arguments:\n",
    "      layer_dims -- List of dimensions, e.g., [784, 128, 10]\n",
    "      \n",
    "    Returns:\n",
    "      parameters -- Dictionary of parameters \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) - 1\n",
    "    for l in range(1, L+1):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1b263f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache keys: ['h0', 'a1', 'h1', 'a2', 'h2']\n",
      "Updated W1 shape: (128, 784)\n"
     ]
    }
   ],
   "source": [
    "# Set up network dimensions and parameters for a 2-layer network\n",
    "layer_dims = [784, 128, 10]\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "optimizer_state = {}  # To store optimizer-specific state variables\n",
    "eta = 0.01           # Learning rate (η)\n",
    "max_epochs = 500\n",
    "t = 1                # Time step (for Adam/Nadam)\n",
    "\n",
    "# Dummy mini-batch (e.g., 32 examples)\n",
    "X_batch = np.random.randn(784, 32)\n",
    "Y_batch = np.zeros((10, 32))\n",
    "for i in range(32):\n",
    "    Y_batch[np.random.randint(0, 10), i] = 1\n",
    "\n",
    "# Forward propagation: Compute activations and store cache\n",
    "AL, act_preact = forward_propagation(X_batch, parameters)\n",
    "\n",
    "print(\"Cache keys:\", list(act_preact.keys()))\n",
    "# Backward propagation: Compute gradients\n",
    "grads = backprop(Y_batch ,act_preact,parameters)\n",
    "\n",
    "# Choose an optimizer, e.g., \"adam\"\n",
    "optimizer = \"adam\"\n",
    "hyperparams = {\"beta1\": 0.9, \"beta2\": 0.999, \"epsilon\": 1e-8, \"beta\": 0.9}  # For optimizers that use beta\n",
    "\n",
    "# Update parameters using the chosen optimizer\n",
    "parameters, optimizer_state = update_parameters(parameters, grads, optimizer, optimizer_state, eta, t, hyperparams)\n",
    "\n",
    "print(\"Updated W1 shape:\", parameters[\"W1\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b93a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86978a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e186d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
